{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6i-pWwA92-rQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ba17cc4-bc6e-4544-8523-f72adc49bec8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved in KushalChandani_kc07535.txt.\n"
          ]
        }
      ],
      "source": [
        "#Question 1\n",
        "\n",
        "import re\n",
        "\n",
        "us_terms = [\"scheduled for\", \"held on\"]\n",
        "uk_terms = [\"registration deadline\", \"due on\", \"final report\"]\n",
        "\n",
        "#Defining the file paths\n",
        "input_file_path = 'date_format_dd_mm_yyyy.txt'\n",
        "output_file_path = 'KushalChandani_kc07535.txt'\n",
        "\n",
        "#Check context and determining date format\n",
        "def determine_date_format(date, context):\n",
        "    day, month, year = map(int, date.split('/'))\n",
        "\n",
        "    #Logical assumptions for non-ambiguous dates\n",
        "    if day > 12:\n",
        "        return \"DD/MM/YYYY\"\n",
        "    elif month > 12:\n",
        "        return \"MM/DD/YYYY\"\n",
        "\n",
        "    #Contextual clues\n",
        "    context = context.lower()\n",
        "    if any(term in context for term in us_terms):\n",
        "        return \"DD/MM/YYYY\"\n",
        "    elif any(term in context for term in uk_terms):\n",
        "        return \"MM/DD/YYYY\"\n",
        "\n",
        "    #If both day and month are â‰¤ 12 and no clear contextual clue, mark as ambiguous\n",
        "    return \"Ambiguous\"\n",
        "\n",
        "#Reading the input file\n",
        "with open(input_file_path, 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "#Extracting dates using regex\n",
        "dates = re.findall(r'\\b\\d{2}/\\d{2}/\\d{4}\\b', text)\n",
        "\n",
        "#Preparing output list\n",
        "output = []\n",
        "\n",
        "#Checking each date and determine its format\n",
        "for date in dates:\n",
        "    #Finding context around the date (50 characters before and after)\n",
        "    context_range = 50\n",
        "    date_index = text.find(date)\n",
        "    context_start = max(0, date_index - context_range)\n",
        "    context_end = min(len(text), date_index + context_range)\n",
        "    context = text[context_start:context_end]\n",
        "\n",
        "    #Determining the date format\n",
        "    format = determine_date_format(date, context)\n",
        "    output.append(f\"{date}: {format}\")\n",
        "\n",
        "#Writing the results to the output file\n",
        "with open(output_file_path, 'w') as file:\n",
        "    for line in output:\n",
        "        file.write(line + '\\n')\n",
        "\n",
        "print(f\"Results saved in {output_file_path}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 2\n",
        "\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "#Tokenizing text with punctuation\n",
        "def tokenize_with_punctuation(text):\n",
        "    return re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)\n",
        "\n",
        "#Reading text from the file and create vocabulary\n",
        "def reading(file_path):\n",
        "    vocab = defaultdict(int)\n",
        "    with open(file_path, 'r') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    tokens = tokenize_with_punctuation(content)\n",
        "\n",
        "    for token in tokens:\n",
        "        vocab[token] += 1\n",
        "    # print(vocab)\n",
        "    return vocab\n",
        "\n",
        "#Computing pair scores based on frequency\n",
        "def compute_pair_scores(splits, vocab):\n",
        "    letter_freqs = defaultdict(int)\n",
        "    pair_freqs = defaultdict(int)\n",
        "\n",
        "    for word, freq in vocab.items():\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            letter_freqs[split[0]] += freq\n",
        "            continue\n",
        "        for i in range(len(split) - 1):\n",
        "            pair = (split[i], split[i + 1])\n",
        "            letter_freqs[split[i]] += freq\n",
        "            pair_freqs[pair] += freq\n",
        "        letter_freqs[split[-1]] += freq\n",
        "\n",
        "    scores = {\n",
        "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
        "        for pair, freq in pair_freqs.items()\n",
        "    }\n",
        "    # print(scores)\n",
        "    return scores\n",
        "\n",
        "#Merging the most frequent pair\n",
        "def merge_pair(a, b, splits, vocab):\n",
        "    for word in vocab:\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "        i = 0\n",
        "        while i < len(split) - 1:\n",
        "            if split[i] == a and split[i + 1] == b:\n",
        "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
        "                split = split[:i] + [merge] + split[i + 2:]\n",
        "            else:\n",
        "                i += 1\n",
        "        splits[word] = split\n",
        "    # print(splits)\n",
        "    return splits\n",
        "\n",
        "#Running the WordPiece algorithm with specified merges\n",
        "def wordpiece_algorithm(file_path, num_merges=10):\n",
        "    vocab = reading(file_path)\n",
        "\n",
        "    splits = {\n",
        "        word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
        "        for word in vocab.keys()\n",
        "    }\n",
        "\n",
        "    alphabet = []\n",
        "    for word in vocab.keys():\n",
        "        if word[0] not in alphabet:\n",
        "            alphabet.append(word[0])\n",
        "        for letter in word[1:]:\n",
        "            if f\"##{letter}\" not in alphabet:\n",
        "                alphabet.append(f\"##{letter}\")\n",
        "\n",
        "    alphabet.sort()\n",
        "\n",
        "    for i in range(num_merges):\n",
        "        scores = compute_pair_scores(splits, vocab)\n",
        "        best_pair, max_score = \"\", None\n",
        "        for pair, score in scores.items():\n",
        "            if max_score is None or max_score < score:\n",
        "                best_pair = pair\n",
        "                max_score = score\n",
        "        splits = merge_pair(*best_pair, splits, vocab)\n",
        "        new_token = (\n",
        "            best_pair[0] + best_pair[1][2:]\n",
        "            if best_pair[1].startswith(\"##\")\n",
        "            else best_pair[0] + best_pair[1]\n",
        "        )\n",
        "        print(f\"Merged: {best_pair} -> {new_token}\")\n",
        "        alphabet.append(new_token)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"wordpiece_input.txt\"\n",
        "    wordpiece_algorithm(file_path, num_merges=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNdURsQCCYsE",
        "outputId": "14bac1ae-2fe5-4288-f008-0184f8975c39"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged: ('1', '##0') -> 10\n",
            "Merged: ('o', '##f') -> of\n",
            "Merged: ('##f', '##y') -> ##fy\n",
            "Merged: ('e', '##x') -> ex\n",
            "Merged: ('##m', '##p') -> ##mp\n",
            "Merged: ('##q', '##u') -> ##qu\n",
            "Merged: ('##b', '##u') -> ##bu\n",
            "Merged: ('##mp', '##l') -> ##mpl\n",
            "Merged: ('##bu', '##l') -> ##bul\n",
            "Merged: ('ex', '##a') -> exa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 3\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "#Reading the Urdu text from the file\n",
        "with open('urdu_text_input.txt', 'r', encoding='utf-8') as file:\n",
        "    urdu_text = file.read()\n",
        "\n",
        "#Tokenization using NLTK\n",
        "def tokenize_nltk(text):\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "#Tokenization using regex\n",
        "def tokenize_regex(text):\n",
        "    return re.findall(r'[\\u0600-\\u06FF]+', text)\n",
        "\n",
        "#Getting tokens from different methods\n",
        "tokens_nltk = tokenize_nltk(urdu_text)\n",
        "tokens_regex = tokenize_regex(urdu_text)\n",
        "\n",
        "#Writing the tokens to output files\n",
        "with open('output_nltk.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write('\\n'.join(tokens_nltk))\n",
        "\n",
        "with open('output_regex.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write('\\n'.join(tokens_regex))"
      ],
      "metadata": {
        "id": "ryQ0bm0hCYpZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eMYm-yoFuV0K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}